{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIT 6.867 Final Project: Ensembling Algorithms\n",
    "Irina Degtiar\n",
    "\n",
    "Sources for functions: https://www.kaggle.com/yekenot/simple-stacker-lb-0-284/code\n",
    "\n",
    "Consider implementing: log odds aggregation: https://www.kaggle.com/aharless/xgboost-k-fold-with-log-odds-aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################\n",
    "### Set up workspace\n",
    "##########################################################################################################\n",
    "# Ensure re-load of all code\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "# Import libraries - general\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Import libraries - classification\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "#from xgboost import XGBClassifier\n",
    "\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "\n",
    "import Helper_files.Gini_coefficient\n",
    "from Helper_files.Gini_coefficient import gini_sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################################################################################\n",
    "### Load data\n",
    "##########################################################################################################\n",
    "training_scaled = pd.read_pickle('../Data/2_Cleaned/training_scaled.pickle')\n",
    "validation_scaled = pd.read_pickle('../Data/2_Cleaned/training_scaled.pickle')\n",
    "test_scaled = pd.read_pickle('../Data/2_Cleaned/training_scaled.pickle')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################################################################################\n",
    "### Prepare data\n",
    "##########################################################################################################\n",
    "X_train = training_scaled.drop(['id', 'target'], 1)\n",
    "y_train = training_scaled['target']\n",
    "\n",
    "X_val = validation_scaled.drop(['id', 'target'], 1)\n",
    "y_val = validation_scaled['target']\n",
    "\n",
    "X_test = test_scaled.drop(['id'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################\n",
    "### Define function - Ensemble running prediction models\n",
    "##########################################################################################################\n",
    "class Ensemble(object):\n",
    "    def __init__(self, n_splits, stacker, base_models):\n",
    "        self.n_splits = n_splits\n",
    "        self.stacker = stacker\n",
    "        self.base_models = base_models\n",
    "\n",
    "    def fit_predict(self, X, y, T):\n",
    "        X = np.array(X) # Train features\n",
    "        y = np.array(y) # Train target values\n",
    "        T = np.array(T) # Test features\n",
    "\n",
    "        folds = list(StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=123).split(X, y))\n",
    "\n",
    "        # Stack predicted probabilities\n",
    "        S_train = np.zeros((X.shape[0], len(self.base_models))) # n_train*#models\n",
    "        S_test = np.zeros((T.shape[0], len(self.base_models)))  # n_test*#models\n",
    "        for i, clf in enumerate(self.base_models):\n",
    "\n",
    "            S_test_i = np.zeros((T.shape[0], self.n_splits)) # n_test*#folds\n",
    "\n",
    "            for j, (train_idx, test_idx) in enumerate(folds):\n",
    "                X_train = X[train_idx]\n",
    "                y_train = y[train_idx]\n",
    "                X_holdout = X[test_idx]\n",
    "#                y_holdout = y[test_idx]\n",
    "\n",
    "                print (\"Fit %s fold %d\" % (str(clf).split('(')[0], j+1))\n",
    "                clf.fit(X_train, y_train)\n",
    "#                cross_score = cross_val_score(clf, X_train, y_train, cv=3, scoring=gini_sklearn)\n",
    "#                print(\"    cross_score: %.5f\" % (cross_score.mean()))\n",
    "                y_pred = clf.predict_proba(X_holdout)[:,1]                \n",
    "\n",
    "                S_train[test_idx, i] = y_pred # Add holdout predictions to appropriate index, dim: n_train*#models\n",
    "                S_test_i[:, j] = clf.predict_proba(T)[:,1] # Predict for training dataset using that fold\n",
    "            S_test[:, i] = S_test_i.mean(axis=1) # Average across folds to get test prediction, dim: n_test*#models\n",
    "\n",
    "        results = cross_val_score(self.stacker, S_train, y, cv=3, scoring=gini_sklearn) # dim: #models\n",
    "        print(\"Stacker score: %.5f\" % (results.mean()))\n",
    "\n",
    "        # Fit ensemble model on stacked training data\n",
    "        self.stacker.fit(S_train, y) # Fit ensemble model: features correspond to holdout predictions from each algorithm in ensemble\n",
    "        res = self.stacker.predict_proba(S_test)[:,1] # Use model to predict targets for test dataset: features correspond to average test prediction across folds from each algorithm in ensemble\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################\n",
    "### Define function - Ensemble using prediction outputs\n",
    "##########################################################################################################\n",
    "# Inputs:\n",
    "#   - stacker: algorithm used to create ensemble prediction\n",
    "#   - S_train: list or array of holdout training predictions from each algorithm in ensemble, dim: n_train*#models\n",
    "#   - S_train: list or array of average test prediction across folds from each algorithm in ensemble, dim: n_test*#models\n",
    "#   - CV: number of cross validations for computing stacker cross_val_score. Default is 3. \n",
    "\n",
    "def Ensemble_from_predictions(stacker, S_train, y_train, S_test, CV=3):\n",
    "    # Clean inputs\n",
    "    S_train = np.array(S_train) # Train predictions\n",
    "    S_test = np.array(S_test) # Test predictions\n",
    "\n",
    "    # Calculate training gini coefficient\n",
    "    results = cross_val_score(stacker, S_train, y_train, cv=CV, scoring=gini_sklearn) # dim: #models\n",
    "    print(\"Stacker score: %.5f\" % (results.mean()))\n",
    "\n",
    "    # Fit ensemble model on stacked training data\n",
    "    stacker.fit(S_train, y) # Fit ensemble model: features correspond to holdout predictions from each algorithm in ensemble\n",
    "    res = stacker.predict_proba(S_test)[:,1] # Use model to predict targets for test dataset: features correspond to average test prediction across folds from each algorithm in ensemble\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################################################################################\n",
    "### Set parameters for each algorithm; create algorithms\n",
    "##########################################################################################################  \n",
    "seed = 123\n",
    "\n",
    "# LightGBM params\n",
    "lgb_params = {}\n",
    "lgb_params['learning_rate'] = 0.02\n",
    "lgb_params['n_estimators'] = 650\n",
    "lgb_params['max_bin'] = 10\n",
    "lgb_params['subsample'] = 0.8\n",
    "lgb_params['subsample_freq'] = 10\n",
    "lgb_params['colsample_bytree'] = 0.8   \n",
    "lgb_params['min_child_samples'] = 500\n",
    "lgb_params['random_state'] = seed\n",
    "\n",
    "\n",
    "lgb_params2 = {}\n",
    "lgb_params2['learning_rate'] = 0.02\n",
    "lgb_params2['n_estimators'] = 1090\n",
    "lgb_params2['colsample_bytree'] = 0.3   \n",
    "lgb_params2['subsample'] = 0.7\n",
    "lgb_params2['subsample_freq'] = 2\n",
    "lgb_params2['num_leaves'] = 16\n",
    "lgb_params2['random_state'] = seed\n",
    "\n",
    "\n",
    "lgb_params3 = {}\n",
    "lgb_params3['learning_rate'] = 0.02\n",
    "lgb_params3['n_estimators'] = 1100\n",
    "lgb_params3['max_depth'] = 4\n",
    "lgb_params3['random_state'] = seed\n",
    "\n",
    "\n",
    "# RandomForest params\n",
    "#rf_params = {}\n",
    "#rf_params['n_estimators'] = 200\n",
    "#rf_params['max_depth'] = 6\n",
    "#rf_params['min_samples_split'] = 70\n",
    "#rf_params['min_samples_leaf'] = 30\n",
    "\n",
    "\n",
    "# ExtraTrees params\n",
    "#et_params = {}\n",
    "#et_params['n_estimators'] = 155\n",
    "#et_params['max_features'] = 0.3\n",
    "#et_params['max_depth'] = 6\n",
    "#et_params['min_samples_split'] = 40\n",
    "#et_params['min_samples_leaf'] = 18\n",
    "\n",
    "\n",
    "# XGBoost params\n",
    "#xgb_params = {}\n",
    "#xgb_params['objective'] = 'binary:logistic'\n",
    "#xgb_params['learning_rate'] = 0.04\n",
    "#xgb_params['n_estimators'] = 490\n",
    "#xgb_params['max_depth'] = 4\n",
    "#xgb_params['subsample'] = 0.9\n",
    "#xgb_params['colsample_bytree'] = 0.9  \n",
    "#xgb_params['min_child_weight'] = 10\n",
    "\n",
    "\n",
    "# Regularized Greedy Forest params\n",
    "#rgf_params = {}\n",
    "#rgf_params['max_leaf'] = 2000\n",
    "#rgf_params['learning_rate'] = 0.5\n",
    "#rgf_params['algorithm'] = \"RGF_Sib\"\n",
    "#rgf_params['test_interval'] = 100\n",
    "#rgf_params['min_samples_leaf'] = 3 \n",
    "#rgf_params['reg_depth'] = 1.0\n",
    "#rgf_params['l2'] = 0.5  \n",
    "#rgf_params['sl2'] = 0.005\n",
    "\n",
    "\n",
    "\n",
    "lgb_model = LGBMClassifier(**lgb_params)\n",
    "lgb_model2 = LGBMClassifier(**lgb_params2)\n",
    "lgb_model3 = LGBMClassifier(**lgb_params3)\n",
    "\n",
    "#rf_model = RandomForestClassifier(**rf_params)\n",
    "#et_model = ExtraTreesClassifier(**et_params)      \n",
    "#xgb_model = XGBClassifier(**xgb_params)\n",
    "#rgf_model = RGFClassifier(**rgf_params) \n",
    "#gb_model = GradientBoostingClassifier(max_depth=5)\n",
    "#ada_model = AdaBoostClassifier()\n",
    "\n",
    "rf_model = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "    max_depth=9, max_features='auto', max_leaf_nodes=None,\n",
    "    #min_impurity_decrease=0, min_impurity_split=None,\n",
    "    min_samples_leaf=1, min_samples_split=2,\n",
    "    min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
    "    oob_score=False, random_state=seed, verbose=0, warm_start=False)\n",
    "nb_model = GaussianNB()\n",
    "\n",
    "log_model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit LGBMClassifier fold 1\n",
      "Fit LGBMClassifier fold 2\n",
      "Fit LGBMClassifier fold 3\n",
      "Fit LGBMClassifier fold 4\n"
     ]
    }
   ],
   "source": [
    "##########################################################################################################\n",
    "### ALT 1: Run stacked ensemble - inputs: models\n",
    "##########################################################################################################\n",
    "stack = Ensemble(n_splits=5,\n",
    "        stacker = log_model,\n",
    "        base_models = (lgb_model, lgb_model2, lgb_model3, rf_model, nb_model))        \n",
    "        \n",
    "y_pred1 = stack.fit_predict(X_train, y_train, X_val)  \n",
    "\n",
    "Helper_files.Gini_coefficient.gini_normalizedc(y_val, y_pred1)\n",
    "# Using lgb1-3, training 3-fold-cross-validation standardized gini coeff: 0.25624, validation: 0.65332241539935254"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################\n",
    "### ALT 2: Run models to obtain predictions (will eventually be done in other files)\n",
    "##########################################################################################################\n",
    "# Input parameters\n",
    "base_models = (lgb_model, lgb_model2, lgb_model3, rf_model, nb_model)\n",
    "n_splits=3\n",
    "\n",
    "X = np.array(X_train) # Train features\n",
    "y = np.array(y_train) # Train target values\n",
    "T = np.array(X_val) # Test features\n",
    "\n",
    "folds = list(StratifiedKFold(n_splits=5, shuffle=True, random_state=123).split(X, y))\n",
    "\n",
    "# Stack predicted probabilities\n",
    "S_train = np.zeros((X.shape[0], len(base_models))) # n_train*#models\n",
    "S_test = np.zeros((T.shape[0], len(base_models)))  # n_test*#models\n",
    "for i, clf in enumerate(base_models):\n",
    "\n",
    "    S_test_i = np.zeros((T.shape[0], n_splits)) # n_test*#folds\n",
    "\n",
    "    for j, (train_idx, test_idx) in enumerate(folds):\n",
    "        X_train_j = X[train_idx]\n",
    "        y_train_j = y[train_idx]\n",
    "        X_holdout_j = X[test_idx]\n",
    "\n",
    "        print (\"Fit %s fold %d\" % (str(clf).split('(')[0], j+1))\n",
    "        clf.fit(X_train_j, y_train_j)\n",
    "        #  cross_score = cross_val_score(clf, X_train_j, y_train_j, cv=3, scoring=gini_sklearn)\n",
    "        #  print(\"    cross_score: %.5f\" % (cross_score.mean()))\n",
    "        y_pred = clf.predict_proba(X_holdout_j)[:,1]                \n",
    "\n",
    "        S_train[test_idx, i] = y_pred # Add holdout predictions to appropriate index, dim: n_train*#models\n",
    "        S_test_i[:, j] = clf.predict_proba(T)[:,1] # Predict for training dataset using that fold\n",
    "    S_test[:, i] = S_test_i.mean(axis=1) # Average across folds to get test prediction, dim: n_test*#models\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################\n",
    "### ALT 2: Run stacked ensemble - inputs: predictions\n",
    "##########################################################################################################\n",
    "stacker=log_model\n",
    "#y_pred2 = Ensemble_from_predictions(stacker, S_train, y, S_test, CV=3) #Works but assumes data already aggregated  \n",
    "\n",
    "# For separate inputs\n",
    "S_train1 = S_train[:,0]\n",
    "S_train2 = S_train[:,1]\n",
    "S_trainmerge = np.array([S_train1,S_train2]).T\n",
    "S_test1 = S_test[:,0]\n",
    "S_test2 = S_test[:,1]\n",
    "S_testmerge = np.array([S_test1,S_test2]).T\n",
    "\n",
    "y_pred2 = Ensemble_from_predictions(stacker=log_model, S_train=S_trainmerge, y_train=y_train, \\\n",
    "                                    S_test=S_testmerge, CV=5)\n",
    "Helper_files.Gini_coefficient.gini_normalizedc(y_val, y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################################################################################\n",
    "### Save predictions for submission\n",
    "##########################################################################################################\n",
    "# submission = pd.DataFrame()\n",
    "# submission['id'] = test_scaled['id']\n",
    "# submission['target'] = y_pred\n",
    "# submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
